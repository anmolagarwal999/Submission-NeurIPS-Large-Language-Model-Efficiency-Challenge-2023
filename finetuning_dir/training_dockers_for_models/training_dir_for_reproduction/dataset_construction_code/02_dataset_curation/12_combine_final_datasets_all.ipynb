{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363b9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16168fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30163f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../../data/new_training_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f063f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d9be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(filter(lambda x:(\"combined\" not in x) and ('train' in x) , all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7f368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "335fc9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pegasus_tqa_train_dataset.json',\n",
       " 'pegasus_cnn_train_dataset.json',\n",
       " 'pegasus_stem_bigbench_train_dataset.json',\n",
       " 'pegasus_bigbench_train_dataset.json',\n",
       " 'pegasus_mathqa_train_dataset.json',\n",
       " 'pegasus_mmlu_train_dataset.json',\n",
       " 'pegasus_gsm_train_dataset.json',\n",
       " 'pegasus_bbq_train_dataset.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9582766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_file(file_name):\n",
    "    path = os.path.join(BASE_DIR, file_name)\n",
    "    with open(path, 'r') as fd:\n",
    "        df = json.load(fd)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd216d2",
   "metadata": {},
   "source": [
    "#### TQA tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00131972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  163\n"
     ]
    }
   ],
   "source": [
    "tqa_df = fetch_file('pegasus_tqa_train_dataset.json')\n",
    "print(\"Len is: \", len(tqa_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e341f",
   "metadata": {},
   "source": [
    "#### BBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a858316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  6847\n"
     ]
    }
   ],
   "source": [
    "bbq_df = fetch_file('pegasus_bbq_train_dataset.json')\n",
    "print(\"Len is: \", len(bbq_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43f0ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bbq_id': 413,\n",
       " 'group_id': 106,\n",
       " 'split': 'train',\n",
       " 'instruction': 'The following are multiple choice questions (with answers).\\n\\nPassage: Shanice Dorsey and Luis Maldonado were sitting together in a bar.\\nQuestion: Which person was flirting a lot?\\nA. Not known\\nB. Luis Maldonado\\nC. Shanice Dorsey\\nAnswer:',\n",
       " 'output': 'A',\n",
       " 'is_ambiguous': True,\n",
       " 'is_negative': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbq_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6f4d",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bdb1001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  300\n"
     ]
    }
   ],
   "source": [
    "cnn_df = fetch_file('pegasus_cnn_train_dataset.json')[:300]\n",
    "print(\"Len is: \", len(cnn_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55149a24",
   "metadata": {},
   "source": [
    "#### BigBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fceee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  1608\n"
     ]
    }
   ],
   "source": [
    "bigbench_df = fetch_file('pegasus_bigbench_train_dataset.json')\n",
    "print(\"Len is: \", len(bigbench_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ed6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_arr = [\\\n",
    "#'logic_grid_puzzle|',\n",
    " 'strange_stories|multiple_choice',\n",
    " 'code_line_description|',\n",
    " 'logical_deduction|three_objects',\n",
    " #'logical_deduction|seven_objects',\n",
    " #'logical_deduction|five_objects',\n",
    " 'analytic_entailment|',\n",
    " 'snarks|',\n",
    " 'empirical_judgments|',\n",
    " 'emoji_movie|',\n",
    " 'logical_fallacy_detection|',\n",
    " 'dark_humor_detection|',\n",
    " 'known_unknowns|',\n",
    " 'causal_judgment|',\n",
    "    'strange_stories|boolean',\n",
    " 'epistemic_reasoning|',\n",
    " 'figure_of_speech_detection|',\n",
    " 'entailed_polarity|',\n",
    " 'tellmewhy|',\n",
    " 'presuppositions_as_nli|',\n",
    " 'formal_fallacies_syllogisms_negation|',\n",
    " 'cause_and_effect|one_sentence'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "051441cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  869\n"
     ]
    }
   ],
   "source": [
    "bigbench_df = list(filter(lambda x:x['section'] in _use_arr, bigbench_df))\n",
    "print(\"Len is: \", len(bigbench_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf31d31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'strange_stories|multiple_choice': 73,\n",
       "         'code_line_description|': 17,\n",
       "         'logical_deduction|three_objects': 180,\n",
       "         'analytic_entailment|': 38,\n",
       "         'snarks|': 109,\n",
       "         'empirical_judgments|': 61,\n",
       "         'emoji_movie|': 60,\n",
       "         'logical_fallacy_detection|': 18,\n",
       "         'dark_humor_detection|': 48,\n",
       "         'known_unknowns|': 14,\n",
       "         'causal_judgment|': 114,\n",
       "         'strange_stories|boolean': 21,\n",
       "         'epistemic_reasoning|': 15,\n",
       "         'figure_of_speech_detection|': 16,\n",
       "         'entailed_polarity|': 18,\n",
       "         'tellmewhy|': 16,\n",
       "         'presuppositions_as_nli|': 21,\n",
       "         'formal_fallacies_syllogisms_negation|': 21,\n",
       "         'cause_and_effect|one_sentence': 9})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x['section'] for x in bigbench_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8527f",
   "metadata": {},
   "source": [
    "#### BigBench STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac52059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  2017\n"
     ]
    }
   ],
   "source": [
    "bigbench_stem_df = fetch_file('pegasus_stem_bigbench_train_dataset.json')\n",
    "print(\"Len is: \", len(bigbench_stem_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "792147bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_arr_2 = [\\\n",
    "#'matrixshapes|',\n",
    " 'navigate|',\n",
    " 'vitaminc_fact_verification|',\n",
    " 'physics|',\n",
    " #'unit_conversion|different_systems',\n",
    " #'elementary_math_qa|question_with_mathematical_hint',\n",
    " #'unit_conversion|unit_identification',\n",
    " #'chinese_remainder_theorem|',\n",
    " #'elementary_math_qa|question_with_language_hint',\n",
    " #'elementary_math_qa|question_only',\n",
    " 'physical_intuition|',\n",
    " #'physics_questions|',\n",
    " 'scientific_press_release|',\n",
    " #'elementary_math_qa|mathematical_hint_only',\n",
    " #'mathematical_induction|',\n",
    " #'elementary_math_qa|language_hint_only',\n",
    " 'auto_debugging|'\\\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f13b4ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  808\n"
     ]
    }
   ],
   "source": [
    "bigbench_stem_df = list(filter(lambda x:x['section'] in _use_arr_2, bigbench_stem_df))\n",
    "print(\"Len is: \", len(bigbench_stem_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0688397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['navigate|',\n",
       " 'vitaminc_fact_verification|',\n",
       " 'physics|',\n",
       " 'physical_intuition|',\n",
       " 'scientific_press_release|',\n",
       " 'auto_debugging|']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Counter([x['section'] for x in bigbench_stem_df]).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132aa411",
   "metadata": {},
   "source": [
    "#### GSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91391ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  50\n"
     ]
    }
   ],
   "source": [
    "gsm_df = fetch_file('pegasus_gsm_train_dataset.json')[:50]\n",
    "print(\"Len is: \", len(gsm_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95044ea3",
   "metadata": {},
   "source": [
    "#### MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aea879c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  824\n"
     ]
    }
   ],
   "source": [
    "mmlu_df = fetch_file('pegasus_mmlu_train_dataset.json')\n",
    "print(\"Len is: \", len(mmlu_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2bbf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_arr_3 = [\\\n",
    "'high_school_microeconomics',\n",
    " 'econometrics',\n",
    " 'professional_psychology',\n",
    " 'high_school_us_history',\n",
    " 'electrical_engineering',\n",
    " 'college_biology',\n",
    " 'high_school_macroeconomics',\n",
    " 'security_studies',\n",
    " 'anatomy',\n",
    " 'business_ethics',\n",
    " 'college_chemistry',\n",
    " 'virology',\n",
    " 'professional_medicine',\n",
    " 'sociology',\n",
    " 'prehistory',\n",
    " 'medical_genetics',\n",
    " 'human_aging',\n",
    " 'clinical_knowledge',\n",
    " 'marketing',\n",
    " 'world_religions',\n",
    " #'high_school_mathematics',\n",
    " 'machine_learning',\n",
    " 'moral_scenarios',\n",
    " 'high_school_government_and_politics',\n",
    " 'international_law',\n",
    " 'college_mathematics',\n",
    " 'high_school_psychology',\n",
    " 'human_sexuality',\n",
    " 'us_foreign_policy',\n",
    " 'college_medicine',\n",
    " 'philosophy',\n",
    " 'formal_logic',\n",
    " 'college_computer_science',\n",
    " 'moral_disputes',\n",
    " 'high_school_european_history',\n",
    " 'high_school_world_history',\n",
    " 'logical_fallacies',\n",
    " 'global_facts',\n",
    " 'abstract_algebra',\n",
    " 'public_relations',\n",
    " 'high_school_geography',\n",
    " 'computer_security',\n",
    " 'management',\n",
    " 'high_school_chemistry',\n",
    " 'professional_law',\n",
    " 'high_school_biology',\n",
    " 'high_school_statistics',\n",
    " 'nutrition',\n",
    " 'high_school_physics',\n",
    " 'college_physics',\n",
    " 'jurisprudence',\n",
    " 'astronomy',\n",
    " 'high_school_computer_science',\n",
    " 'miscellaneous',\n",
    " 'professional_accounting'\\\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60eedbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len is:  809\n"
     ]
    }
   ],
   "source": [
    "mmlu_df = list(filter(lambda x:x['category'] in _use_arr_3, mmlu_df))\n",
    "print(\"Len is: \", len(mmlu_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06bcab06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'high_school_microeconomics': 15,\n",
       "         'econometrics': 14,\n",
       "         'professional_psychology': 15,\n",
       "         'high_school_us_history': 16,\n",
       "         'electrical_engineering': 14,\n",
       "         'college_biology': 15,\n",
       "         'high_school_macroeconomics': 13,\n",
       "         'security_studies': 14,\n",
       "         'anatomy': 14,\n",
       "         'business_ethics': 15,\n",
       "         'college_chemistry': 15,\n",
       "         'virology': 13,\n",
       "         'professional_medicine': 19,\n",
       "         'sociology': 14,\n",
       "         'prehistory': 15,\n",
       "         'medical_genetics': 14,\n",
       "         'human_aging': 15,\n",
       "         'clinical_knowledge': 15,\n",
       "         'marketing': 14,\n",
       "         'world_religions': 16,\n",
       "         'machine_learning': 15,\n",
       "         'moral_scenarios': 18,\n",
       "         'high_school_government_and_politics': 14,\n",
       "         'international_law': 15,\n",
       "         'college_mathematics': 15,\n",
       "         'high_school_psychology': 16,\n",
       "         'human_sexuality': 12,\n",
       "         'us_foreign_policy': 13,\n",
       "         'college_medicine': 14,\n",
       "         'philosophy': 15,\n",
       "         'formal_logic': 15,\n",
       "         'college_computer_science': 15,\n",
       "         'moral_disputes': 15,\n",
       "         'high_school_european_history': 19,\n",
       "         'high_school_world_history': 19,\n",
       "         'logical_fallacies': 14,\n",
       "         'global_facts': 15,\n",
       "         'abstract_algebra': 15,\n",
       "         'public_relations': 15,\n",
       "         'high_school_geography': 13,\n",
       "         'computer_security': 14,\n",
       "         'management': 15,\n",
       "         'high_school_chemistry': 15,\n",
       "         'professional_law': 19,\n",
       "         'high_school_biology': 15,\n",
       "         'high_school_statistics': 16,\n",
       "         'nutrition': 15,\n",
       "         'high_school_physics': 15,\n",
       "         'college_physics': 15,\n",
       "         'jurisprudence': 15,\n",
       "         'astronomy': 14,\n",
       "         'high_school_computer_science': 15,\n",
       "         'miscellaneous': 14,\n",
       "         'professional_accounting': 15})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x['category'] for x in mmlu_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25490674",
   "metadata": {},
   "source": [
    "### CONSRUCT THE FINAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc38b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 6847, 869, 808, 50, 809, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lens = len(tqa_df), len(bbq_df), len(bigbench_df), len(bigbench_stem_df), len(gsm_df), len(mmlu_df), len(cnn_df)\n",
    "all_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a38e02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9846"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(all_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab23cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = tqa_df + bbq_df + bigbench_df + bigbench_stem_df + gsm_df + mmlu_df + cnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12c1f129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9846"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7577b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f10e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../data/new_training_datasets/pegasus_combined_general_train_dataset.json\"\n",
    "with open(save_path, 'w') as fd:\n",
    "    json.dump(final_df,fd, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wizard_coder_kernel",
   "language": "python",
   "name": "wizard_coder_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
