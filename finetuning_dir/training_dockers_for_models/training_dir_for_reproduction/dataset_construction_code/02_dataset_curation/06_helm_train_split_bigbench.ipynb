{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00a4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "from collections import Counter\n",
    "import importlib\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb73209",
   "metadata": {},
   "source": [
    "/home/anmol/nips_challenge/efficiency_challenge_repo/external_repos/helm/src/helm/benchmark/scenarios/bbq_scenario.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5afca",
   "metadata": {},
   "source": [
    "#### Used in:\n",
    "* Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eac0e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e83de3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/anmol/nips_challenge/efficiency_challenge_repo/external_repos/helm/src/helm/benchmark/scenarios')\n",
    "sys.path.append('/home/anmol/nips_challenge/efficiency_challenge_repo/external_repos/helm/src/helm/benchmark/augmentations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f752c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16805330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils_code.helper' from '/home/anmol/nips_challenge/efficiency_challenge_repo/code/02_dataset_curation/../utils_code/helper.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils_code.helper as helper\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9baa260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helm\n",
    "from helm.benchmark.scenarios.bbq_scenario import BBQScenario\n",
    "from helm.benchmark.scenarios.truthful_qa_scenario import TruthfulQAScenario\n",
    "from helm.benchmark.scenarios.mmlu_scenario import MMLUScenario\n",
    "from helm.benchmark.scenarios.summarization_scenario import SummarizationScenario\n",
    "from helm.benchmark.scenarios.big_bench_scenario import BIGBenchScenario\n",
    "\n",
    "\n",
    "from helm.benchmark.scenarios.scenario import Scenario, ScenarioSpec\n",
    "\n",
    "\n",
    "from helm.benchmark.augmentations.mild_mix_perturbation import MildMixPerturbation\n",
    "from helm.benchmark.augmentations.dialect_perturbation import DialectPerturbation\n",
    "from helm.benchmark.augmentations.gender_perturbation import GenderPerturbation\n",
    "from helm.benchmark.augmentations.person_name_perturbation import PersonNamePerturbation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80750d80",
   "metadata": {},
   "source": [
    "####  BigBench  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a77da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigbench_tasks = {\n",
    "    \"analytic_entailment\":[],\n",
    "\"causal_judgment\":[],\n",
    "\"emoji_movie\":[],\n",
    "\"empirical_judgments\":[],\n",
    "\"known_unknowns\":[],\n",
    "\"logical_deduction\":[\"three_objects\", \"five_objects\", \"seven_objects\"],\n",
    "\"strange_stories\":[\"multiple_choice\", \"boolean\"],\n",
    "#\"logical_deduction\":[\"three_objects\"],\n",
    "#\"strange_stories\":[\"multiple_choice\"],\n",
    "\"snarks\":[],\n",
    "\"dark_humor_detection\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5291597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tasks_dict = {\n",
    "    \"causal_judgment\": [],\n",
    "    \"cause_and_effect\": [\n",
    "                'one_sentence'\n",
    "              ],\n",
    "    \"com2sense\": [],\n",
    "    \"empirical_judgments\": [],\n",
    "    \"entailed_polarity\": [],\n",
    "    #\"entailed_polarity_hindi\": [],\n",
    "    #\"fantasy_reasoning\": [],\n",
    "    \"figure_of_speech_detection\": [],\n",
    "    \"forecasting_subquestions\": [],\n",
    "    #\"goal_step_wikihow\": [],\n",
    "    #\"human_organs_senses\": [],\n",
    "    #\"indic_cause_and_effect\": [],\n",
    "    \"minute_mysteries_qa\": [],\n",
    "    #\"moral_permissibility\": [],\n",
    "    #\"simple_ethical_questions\": [],\n",
    "    \"tellmewhy\": [],\n",
    "    #\"winowhy\": [],\n",
    "    \"analytic_entailment\": [],\n",
    "    #\"auto_categorization\": [],\n",
    "    #\"auto_debugging\": [],\n",
    "    \"boolean_expressions\": [],\n",
    "    #\"checkmate_in_one\": [],\n",
    "    #\"chess_state_tracking\": [],\n",
    "    \"code_line_description\": [],\n",
    "    #\"codenames\": [],\n",
    "    #\"conlang_translation\": [],\n",
    "    \"context_definition_alignment\": [],\n",
    "    #\"crass_ai\": [],\n",
    "    #\"cryptonite\": [],\n",
    "    #\"date_understanding\": [],\n",
    "    #\"dyck_languages\": [],\n",
    "    #\"dynamic_counting\": [],\n",
    "    #\"elementary_math_qa\": [],\n",
    "    \"epistemic_reasoning\": [],\n",
    "    #\"evaluating_information_essentiality\": [],\n",
    "    \"formal_fallacies_syllogisms_negation\": [],\n",
    "    #\"gender_sensitivity_chinese\": [],\n",
    "    \"gender_sensitivity_english\": [],\n",
    "    #\"gre_reading_comprehension\": [],\n",
    "    #\"identify_math_theorems\": [],\n",
    "    #\"intersect_geometry\": [],\n",
    "    #\"kannada\": [],\n",
    "    #\"key_value_maps\": [],\n",
    "    \"language_games\": [],\n",
    "    #\"linguistics_puzzles\": [],\n",
    "    \"logic_grid_puzzle\": [],\n",
    "    #\"logical_args\": [],\n",
    "    \"logical_deduction\": [],\n",
    "    \"logical_fallacy_detection\": [],\n",
    "    #\"navigate\": [],\n",
    "    #\"nonsense_words_grammar\": [],\n",
    "    \"object_counting\": [],\n",
    "    #\"penguins_in_a_table\": [],\n",
    "    #\"physics_questions\": [],\n",
    "    \"presuppositions_as_nli\": [],\n",
    "    #\"program_synthesis\": [],\n",
    "    #\"reasoning_about_colored_objects\": [],\n",
    "    #\"repeat_copy_logic\": [],\n",
    "    #\"rephrase\": [],\n",
    "    #\"roots_optimization_and_games\": [],\n",
    "    \"simp_turing_concept\": [],\n",
    "    #\"simple_text_editing\": [],\n",
    "    \"spelling_bee\": [],\n",
    "    #\"strategyqa\": [],\n",
    "    \"sudoku\": [],\n",
    "    #\"sufficient_information\": [],\n",
    "    #\"symbol_interpretation\": [],\n",
    "    #\"temporal_sequences\": [],\n",
    "    #\"timedial\": [],\n",
    "    \"tracking_shuffled_objects\": [],\n",
    "    #\"undo_permutation\": [],\n",
    "    #\"unit_interpretation\": [],\n",
    "    #\"web_of_lies\": [],\n",
    "    #\"word_problems_on_sets_and_graphs\": []\n",
    "}\n",
    "_keys = list(extra_tasks_dict.keys())\n",
    "for _key in _keys:\n",
    "    if _key in bigbench_tasks:\n",
    "        del extra_tasks_dict[_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de400f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/analytic_entailment/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/analytic_entailment/analytic_entailment/task.json already exists\n",
      "} [0.001s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 38, 'test': 16, 'valid': 16})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/causal_judgment/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/causal_judgment/causal_judgment/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 114, 'test': 38, 'valid': 38})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/emoji_movie/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/emoji_movie/emoji_movie/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 60, 'test': 20, 'valid': 20})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/empirical_judgments/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/empirical_judgments/empirical_judgments/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 61, 'test': 19, 'valid': 19})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/known_unknowns/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/known_unknowns/known_unknowns/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'test': 16, 'valid': 16, 'train': 14})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/three_objects/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_deduction/three_objects/logical_deduction/three_objects/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 180, 'test': 60, 'valid': 60})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/five_objects/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_deduction/five_objects/logical_deduction/five_objects/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 300, 'test': 100, 'valid': 100})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/seven_objects/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/logical_deduction/seven_objects/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 420, 'test': 140, 'valid': 140})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/multiple_choice/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/strange_stories/multiple_choice/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 73, 'test': 24, 'valid': 24})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/boolean/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/strange_stories/boolean/strange_stories/boolean/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 21, 'test': 16, 'valid': 16})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/snarks/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/snarks/snarks/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 109, 'test': 36, 'valid': 36})\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/dark_humor_detection/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/dark_humor_detection/dark_humor_detection/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 48, 'test': 16, 'valid': 16})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ans_df = dict()\n",
    "\n",
    "for curr_task in bigbench_tasks:\n",
    "    if len(bigbench_tasks[curr_task])==0:\n",
    "        bigbench_tasks[curr_task]=  [\"\"]\n",
    "    for subtask in bigbench_tasks[curr_task]:\n",
    "        big_bench_obj = BIGBenchScenario(curr_task, subtask)\n",
    "        if subtask == \"\":\n",
    "            big_bench_obj.output_path = f\"../01_helm_eval_setup/benchmark_output/scenarios/big_bench/{curr_task}\"\n",
    "        else:\n",
    "            big_bench_obj.output_path = f\"../01_helm_eval_setup/benchmark_output/scenarios/big_bench/{curr_task}/{subtask}\"\n",
    "        ans = big_bench_obj.get_instances(big_bench_obj.output_path)\n",
    "        ans = list(filter(lambda x:helper.fetch_num_correct_options(x)==1, ans))\n",
    "        _tag  = curr_task+\"|\"+subtask\n",
    "        ans_df[_tag] = ans\n",
    "        print(\"TRAIN-TEST SPLIT are: \", Counter([x.split for x in ans]))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84bfd985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../01_helm_eval_setup/benchmark_output/scenarios/big_bench/dark_humor_detection'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_bench_obj.output_path"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d652bd95",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63d7d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "TASK is:  cause_and_effect\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/cause_and_effect/one_sentence/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/cause_and_effect/one_sentence/cause_and_effect/one_sentence/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'valid': 11, 'test': 10, 'train': 9})\n",
      "##########\n",
      "TASK is:  com2sense\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/com2sense/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/com2sense/com2sense/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  Expecting value: line 1 column 1 (char 0)\n",
      "##########\n",
      "TASK is:  entailed_polarity\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/entailed_polarity/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/entailed_polarity/entailed_polarity/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 18, 'test': 6, 'valid': 6})\n",
      "##########\n",
      "TASK is:  figure_of_speech_detection\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/figure_of_speech_detection/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/figure_of_speech_detection/figure_of_speech_detection/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 16, 'test': 9, 'valid': 5})\n",
      "##########\n",
      "TASK is:  forecasting_subquestions\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/forecasting_subquestions/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/forecasting_subquestions/forecasting_subquestions/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  Expecting value: line 1 column 1 (char 0)\n",
      "##########\n",
      "TASK is:  minute_mysteries_qa\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/minute_mysteries_qa/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/minute_mysteries_qa/minute_mysteries_qa/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  'examples'\n",
      "##########\n",
      "TASK is:  tellmewhy\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/tellmewhy/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/tellmewhy/tellmewhy/task.json already exists\n",
      "} [0.001s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 16, 'test': 11, 'valid': 3})\n",
      "##########\n",
      "TASK is:  boolean_expressions\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/boolean_expressions/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/boolean_expressions/boolean_expressions/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  Expecting value: line 1 column 1 (char 0)\n",
      "##########\n",
      "TASK is:  code_line_description\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/code_line_description/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/code_line_description/code_line_description/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 17, 'test': 8, 'valid': 5})\n",
      "##########\n",
      "TASK is:  context_definition_alignment\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/context_definition_alignment/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/context_definition_alignment/context_definition_alignment/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  Expecting value: line 1 column 1 (char 0)\n",
      "##########\n",
      "TASK is:  epistemic_reasoning\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/epistemic_reasoning/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/epistemic_reasoning/epistemic_reasoning/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 15, 'test': 8, 'valid': 7})\n",
      "##########\n",
      "TASK is:  formal_fallacies_syllogisms_negation\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/formal_fallacies_syllogisms_negation/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/formal_fallacies_syllogisms_negation/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 21, 'valid': 5, 'test': 4})\n",
      "##########\n",
      "TASK is:  gender_sensitivity_english\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/gender_sensitivity_english/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/gender_sensitivity_english/gender_sensitivity_english/task.json already exists\n",
      "} [0.001s]\n",
      "Error is:  Expecting value: line 1 column 1 (char 0)\n",
      "##########\n",
      "TASK is:  language_games\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/language_games/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/language_games/language_games/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  'examples'\n",
      "##########\n",
      "TASK is:  logic_grid_puzzle\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logic_grid_puzzle/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logic_grid_puzzle/logic_grid_puzzle/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 19, 'test': 7, 'valid': 4})\n",
      "##########\n",
      "TASK is:  logical_fallacy_detection\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_fallacy_detection/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_fallacy_detection/logical_fallacy_detection/task.json already exists\n",
      "} [0.001s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 18, 'test': 8, 'valid': 4})\n",
      "##########\n",
      "TASK is:  object_counting\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/object_counting/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/object_counting/object_counting/task.json already exists\n",
      "} [0.001s]\n",
      "TRAIN-TEST SPLIT are:  Counter()\n",
      "##########\n",
      "TASK is:  presuppositions_as_nli\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/presuppositions_as_nli/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/presuppositions_as_nli/presuppositions_as_nli/task.json already exists\n",
      "} [0.0s]\n",
      "TRAIN-TEST SPLIT are:  Counter({'train': 21, 'valid': 5, 'test': 4})\n",
      "##########\n",
      "TASK is:  simp_turing_concept\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/simp_turing_concept/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/simp_turing_concept/simp_turing_concept/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  'examples'\n",
      "##########\n",
      "TASK is:  spelling_bee\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/spelling_bee/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/spelling_bee/spelling_bee/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  Expecting value: line 1 column 1 (char 0)\n",
      "##########\n",
      "TASK is:  sudoku\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/sudoku/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/sudoku/sudoku/task.json already exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "} [0.0s]\n",
      "Error is:  Expecting value: line 1 column 1 (char 0)\n",
      "##########\n",
      "TASK is:  tracking_shuffled_objects\n",
      "ensure_file_downloaded {\n",
      "  Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/tracking_shuffled_objects/task.json because ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/tracking_shuffled_objects/tracking_shuffled_objects/task.json already exists\n",
      "} [0.0s]\n",
      "Error is:  'examples'\n"
     ]
    }
   ],
   "source": [
    "dict_to_use = extra_tasks_dict\n",
    "for curr_task in dict_to_use:\n",
    "    print(\"##########\")\n",
    "    print(\"TASK is: \", curr_task)\n",
    "    try:\n",
    "        if len(dict_to_use[curr_task])==0:\n",
    "            dict_to_use[curr_task]=  [\"\"]\n",
    "        for subtask in dict_to_use[curr_task]:\n",
    "            big_bench_obj = BIGBenchScenario(curr_task, subtask)\n",
    "            if subtask == \"\":\n",
    "                big_bench_obj.output_path = f\"../01_helm_eval_setup/benchmark_output/scenarios/big_bench/{curr_task}\"\n",
    "            else:\n",
    "                big_bench_obj.output_path = f\"../01_helm_eval_setup/benchmark_output/scenarios/big_bench/{curr_task}/{subtask}\"\n",
    "            ans = big_bench_obj.get_instances(big_bench_obj.output_path)\n",
    "            ans = list(filter(lambda x:helper.fetch_num_correct_options(x)==1, ans))\n",
    "            random.shuffle(ans)\n",
    "            ans = ans[:30]\n",
    "            _tag  = curr_task+\"|\"+subtask\n",
    "            ans_df[_tag] = ans\n",
    "            print(\"TRAIN-TEST SPLIT are: \", Counter([x.split for x in ans]))\n",
    "    except Exception as E:\n",
    "        print(\"Error is: \", E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e9fc47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  analytic_entailment|  | Split:  Counter({'train': 38, 'test': 16, 'valid': 16})\n",
      "=====\n",
      "######\n",
      "Key:  causal_judgment|  | Split:  Counter({'train': 114, 'test': 38, 'valid': 38})\n",
      "=====\n",
      "######\n",
      "Key:  emoji_movie|  | Split:  Counter({'train': 60, 'test': 20, 'valid': 20})\n",
      "=====\n",
      "######\n",
      "Key:  empirical_judgments|  | Split:  Counter({'train': 61, 'test': 19, 'valid': 19})\n",
      "=====\n",
      "######\n",
      "Key:  known_unknowns|  | Split:  Counter({'test': 16, 'valid': 16, 'train': 14})\n",
      "=====\n",
      "######\n",
      "Key:  logical_deduction|three_objects  | Split:  Counter({'train': 180, 'test': 60, 'valid': 60})\n",
      "=====\n",
      "######\n",
      "Key:  logical_deduction|five_objects  | Split:  Counter({'train': 300, 'test': 100, 'valid': 100})\n",
      "=====\n",
      "######\n",
      "Key:  logical_deduction|seven_objects  | Split:  Counter({'train': 420, 'test': 140, 'valid': 140})\n",
      "=====\n",
      "######\n",
      "Key:  strange_stories|multiple_choice  | Split:  Counter({'train': 73, 'test': 24, 'valid': 24})\n",
      "=====\n",
      "######\n",
      "Key:  strange_stories|boolean  | Split:  Counter({'train': 21, 'test': 16, 'valid': 16})\n",
      "=====\n",
      "######\n",
      "Key:  snarks|  | Split:  Counter({'train': 109, 'test': 36, 'valid': 36})\n",
      "=====\n",
      "######\n",
      "Key:  dark_humor_detection|  | Split:  Counter({'train': 48, 'test': 16, 'valid': 16})\n",
      "=====\n",
      "######\n",
      "Key:  cause_and_effect|one_sentence  | Split:  Counter({'valid': 11, 'test': 10, 'train': 9})\n",
      "=====\n",
      "######\n",
      "Key:  entailed_polarity|  | Split:  Counter({'train': 18, 'test': 6, 'valid': 6})\n",
      "=====\n",
      "######\n",
      "Key:  figure_of_speech_detection|  | Split:  Counter({'train': 16, 'test': 9, 'valid': 5})\n",
      "=====\n",
      "######\n",
      "Key:  tellmewhy|  | Split:  Counter({'train': 16, 'test': 11, 'valid': 3})\n",
      "=====\n",
      "######\n",
      "Key:  code_line_description|  | Split:  Counter({'train': 17, 'test': 8, 'valid': 5})\n",
      "=====\n",
      "######\n",
      "Key:  epistemic_reasoning|  | Split:  Counter({'train': 15, 'test': 8, 'valid': 7})\n",
      "=====\n",
      "######\n",
      "Key:  formal_fallacies_syllogisms_negation|  | Split:  Counter({'train': 21, 'valid': 5, 'test': 4})\n",
      "=====\n",
      "######\n",
      "Key:  logic_grid_puzzle|  | Split:  Counter({'train': 19, 'test': 7, 'valid': 4})\n",
      "=====\n",
      "######\n",
      "Key:  logical_fallacy_detection|  | Split:  Counter({'train': 18, 'test': 8, 'valid': 4})\n",
      "=====\n",
      "######\n",
      "Key:  object_counting|  | Split:  Counter()\n",
      "=====\n",
      "######\n",
      "Key:  presuppositions_as_nli|  | Split:  Counter({'train': 21, 'valid': 5, 'test': 4})\n",
      "=====\n",
      "######\n"
     ]
    }
   ],
   "source": [
    "for curr_key, curr_val in ans_df.items():\n",
    "    print(\"Key: \", curr_key, \" | Split: \", Counter([x.split for x in curr_val]) )\n",
    "    print('=====')\n",
    "    #print(curr_val[0].input.text)\n",
    "    print(\"######\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75a9d389",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a16b97",
   "metadata": {},
   "source": [
    "```python\n",
    "adapter_spec = AdapterSpec(\n",
    "        method=get_adaptation_method(big_bench_task[\"metrics\"]),\n",
    "        model=\"openai/text-curie-001\",  # Can override with the `ModelRunExpander`.\n",
    "        max_train_instances=5,  # Can override with the `MaxTrainInstancesRunExpander`.\n",
    "        num_outputs=1,  # Can override with the `NumOutputsRunExpander`.\n",
    "        # From \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\",\n",
    "        # for the BIG-G models tested on BIG-bench, \"we use an input context length of 1,024 tokens\n",
    "        # and an output length of 64 tokens. We evaluate on up to 1,000 examples per task\".\n",
    "        max_tokens=64,\n",
    "        # \"all model outputs were sampled greedily (with zero temperature), unless otherwise noted.\"\n",
    "        temperature=0,\n",
    "        instructions=big_bench_task.get(\"task_prefix\", \"\"),\n",
    "        # BIG-bench's default value for \"example_input_prefix\" and \"example_output_prefix\" was \"\\nQ: \" and \"\\nA: \".\n",
    "        # Instead, use our defaults for multiple choice tasks: \"Question: \" and \"\\nAnswer: \".\n",
    "        input_prefix=big_bench_task.get(\"example_input_prefix\", \"Question: \"),\n",
    "        output_prefix=big_bench_task.get(\"example_output_prefix\", \"Answer: \"),\n",
    "        # Use our default for multiple choice: A., B., C., D.,...\n",
    "        # reference_prefix=big_bench_task.get(\"choice_prefix\", \"\\n choice: \"),\n",
    "        # The default value for \"stop_string\" in BIG-bench is None.\n",
    "        stop_sequences=[str(big_bench_task.get(\"stop_string\"))] if big_bench_task.get(\"stop_string\", None) else [],\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c877a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../01_helm_eval_setup/benchmark_output/scenarios/big_bench\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e61be",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Determine whether the following pairs of sentences embody an entailment relation or not.\n",
    "\n",
    "\n",
    "Sentences: Sally met two actresses. So Sally met at least one woman.\n",
    "A. entailment\n",
    "B. no-entailment\n",
    "\n",
    "Relation: A\n",
    "\n",
    "\n",
    "Sentences: Lina met two nurses. So, Lina met at least one woman.\n",
    "A. entailment\n",
    "B. no-entailment\n",
    "\n",
    "Relation: B\n",
    "\n",
    "\n",
    "Sentences: The counter is made of cherry wood. So, the counter is wooden.\n",
    "A. entailment\n",
    "B. no-entailment\n",
    "\n",
    "Relation: A\n",
    "\n",
    "\n",
    "Sentences: Mary has a beautiful garden. So Mary is a gardener.\n",
    "A. entailment\n",
    "B. no-entailment\n",
    "\n",
    "Relation:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60894f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49fad5d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################\n",
      "-------  analytic_entailment|  --------- \n",
      "analytic_entailment  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/analytic_entailment/analytic_entailment/task.json\n",
      "Determine whether the following pairs of sentences embody an entailment relation or not.\n",
      "\n",
      "Sentences: Tom is George‚Äôs brother. So, George is a descendant of Tom‚Äôs.\n",
      "A. entailment\n",
      "B. no-entailment\n",
      "\n",
      "Relation: \n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  causal_judgment|  --------- \n",
      "causal_judgment  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/causal_judgment/causal_judgment/task.json\n",
      "How would a typical person answer each of the following questions about causation?\n",
      "\n",
      "Question: Tom has a huge garden and loves flowers. He employed two gardeners who take care of the plants on his 30 flower beds: Alex and Benni. Both can independently decide on their working hours and arrange who cares for which flower beds. Alex and Benni are very reliable and Tom is satisfied with their work. Nevertheless he wants to optimize the plant growth. Since Tom has read in a magazine that plants grow better when they are fertilized, he decides to let Alex and Benni fertilize his plants. The magazine recommends the use of the chemicals A X200R or B Y33R, since both are especially effective. However, Tom also read that it can damage plants when they are exposed to multiple different types of chemicals. Tom therefore decides that he only wants to use one fertilizer. He goes for A X200R. Tom instructs Alex and Benni to buy the chemical A X200R and to use only this fertilizer. Alex volunteers for buying several bottles of this chemical for Benni and himself. After a few weeks, Tom goes for a walk in his garden. He realizes that some of his plants are much prettier and bigger than before. However, he also realizes that some of his plants have lost their beautiful color and are dried up. That makes Tom very sad and reflective. He wonders whether the drying of his plants might have something to do with the fertilization. He wants to investigate this matter and talks to Alex and Benni. Alex tells him that he followed Tom's instructions and only bought and used the chemical A X200R. However, Benni tells him that he had used the chemical B Y33R instead. He still had some bottles of this chemical in stock at home and wanted to use them up. Tom realizes that the plants dried up in the flower beds on which both A X200R and B Y33R were applied by the gardeners. Did Benni cause the plant to dry out?\n",
      "A. Yes\n",
      "B. No\n",
      "Answer: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  emoji_movie|  --------- \n",
      "emoji_movie  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/emoji_movie/emoji_movie/task.json\n",
      "Question: What movie does this emoji describe? üì¨üë©‚Äç‚ù§Ô∏è‚Äçüë®\n",
      "A. you've got mail\n",
      "B. the thing\n",
      "C. the truman show\n",
      "D. inception\n",
      "E. snatch.\n",
      "Answer: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  empirical_judgments|  --------- \n",
      "empirical_judgments  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/empirical_judgments/empirical_judgments/task.json\n",
      "Determine whether a given sentence asserts a causal, correlative, or neutral relation between two events. If the sentence asserts a causal relation respond causal, if the sentence asserts a correlative relation respond correlative, if the sentence asserts neither a causal nor a correlative relation between two events respond neutral.\n",
      "\n",
      "Sentence: Washing your hands reduces the risk of infection.\n",
      "A. causal\n",
      "B. correlative\n",
      "C. neutral\n",
      "\n",
      "Relation: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  known_unknowns|  --------- \n",
      "known_unknowns  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/known_unknowns/known_unknowns/task.json\n",
      "Question: How much food does the cat Tinyman Zhengel eat every day?\n",
      "A. 250 calories\n",
      "B. Unknown\n",
      "Answer: \n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  logical_deduction|three_objects  --------- \n",
      "logical_deduction three_objects ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_deduction/three_objects/logical_deduction/three_objects/task.json\n",
      "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In an antique car show, there are three vehicles: a truck, a minivan, and a tractor. The truck is newer than the minivan. The minivan is newer than the tractor.\n",
      "A. The truck is the oldest.\n",
      "B. The minivan is the oldest.\n",
      "C. The tractor is the oldest.\n",
      " \n",
      "OUTPUT::: C\n",
      "#################\n",
      "-------  logical_deduction|five_objects  --------- \n",
      "logical_deduction five_objects ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_deduction/five_objects/logical_deduction/five_objects/task.json\n",
      "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "A fruit stand sells five fruits: pears, mangoes, kiwis, oranges, and peaches. The peaches are more expensive than the mangoes. The oranges are more expensive than the kiwis. The pears are the most expensive. The mangoes are more expensive than the oranges.\n",
      "A. The pears are the third-most expensive.\n",
      "B. The mangoes are the third-most expensive.\n",
      "C. The kiwis are the third-most expensive.\n",
      "D. The oranges are the third-most expensive.\n",
      "E. The peaches are the third-most expensive.\n",
      " \n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  logical_deduction|seven_objects  --------- \n",
      "logical_deduction seven_objects ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/logical_deduction/seven_objects/task.json\n",
      "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n",
      "\n",
      "In a golf tournament, there were seven golfers: Eli, Ada, Amy, Ana, Eve, Mel, and Dan. Ada finished above Mel. Dan finished above Ada. Amy finished last. Ana finished third-to-last. Dan finished below Eli. Eve finished third.\n",
      "A. Eli finished third.\n",
      "B. Ada finished third.\n",
      "C. Amy finished third.\n",
      "D. Ana finished third.\n",
      "E. Eve finished third.\n",
      "F. Mel finished third.\n",
      "G. Dan finished third.\n",
      " \n",
      "OUTPUT::: E\n",
      "#################\n",
      "-------  strange_stories|multiple_choice  --------- \n",
      "strange_stories multiple_choice ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/strange_stories/multiple_choice/task.json\n",
      "Context: Sam decides to go on a long walk to get some fresh air. Unfortunately, just after leaving the house, the wind begins to pick up and it starts to rain. Luckily Sam always has an umbrella with him. He quickly puts up the umbrella and wraps his coat tightly around him. Suddenly a gust of wind blows the umbrella straight out of Sam's hand and it lands in a large, very prickly bush. Sam manages to run and fetch it before it blows off again and is pleased to find it all in one piece. As he walks home, he notices that his head is starting to get wet despite the umbrella. Q: Why is Sam getting wet?\n",
      "A. The umbrella broke because of the wind.\n",
      "B. The umbrella has holes from landing in the bush.\n",
      "C. The umbrella has holes.\n",
      "D. The umbrella was too small.\n",
      "\n",
      "A: \n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  strange_stories|boolean  --------- \n",
      "strange_stories boolean ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/strange_stories/boolean/strange_stories/boolean/task.json\n",
      "Context: During the war, the Red army captures a member of the Blue army. They want him to tell them where his army's tanks are; they know they are either by the sea or in the mountains. They know that the prisoner will not want to tell them, he will want to save his army, and so he will certainly lie to them. The prisoner is very brave and very clever, he will not let them find his tanks. The tanks are really in the mountains. Now when the other side ask him where his tanks are, he says, \"They are in the mountains\".\n",
      "Q: Does the prisoner know where the tanks are?\n",
      "A. yes\n",
      "B. no\n",
      "\n",
      "A: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  snarks|  --------- \n",
      "snarks  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/snarks/snarks/task.json\n",
      "Question: Which statement is sarcastic? (a) conspiracy theory, sounds suspicious (b) conspiracy theory, sounds legit\n",
      "A. (a)\n",
      "B. (b)\n",
      "Answer: \n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  dark_humor_detection|  --------- \n",
      "dark_humor_detection  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/dark_humor_detection/dark_humor_detection/task.json\n",
      "Identify whether the following examples are intended to be a joke (with dark humor) or not, by responding \"joke\" or \"not a joke\".\n",
      "\n",
      "Example: It's important to have a good vocabulary. I wish I had known the difference between the words 'antidote' and 'anecdote.' It was on the quiz today at my English class.\n",
      "A. joke\n",
      "B. not a joke\n",
      "\n",
      "Joke or not? \n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  cause_and_effect|one_sentence  --------- \n",
      "cause_and_effect one_sentence ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/cause_and_effect/one_sentence/cause_and_effect/one_sentence/task.json\n",
      "Which of the following sentences makes more sense?\n",
      "example:\n",
      "A. The company's stock went up because the company's posted strong earnings.\n",
      "B. The company's posted strong earnings because the company's stock went up.\n",
      "\n",
      "answer: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  entailed_polarity|  --------- \n",
      "entailed_polarity  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/entailed_polarity/entailed_polarity/task.json\n",
      "Given a fact, answer the following question with a yes or a no.\n",
      "Fact: Ed got to write to the doctor. Q: Did Ed write to the doctor?\n",
      "A. no\n",
      "B. yes\n",
      "Answer: \n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  figure_of_speech_detection|  --------- \n",
      "figure_of_speech_detection  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/figure_of_speech_detection/figure_of_speech_detection/task.json\n",
      "Please identify the figure of speech embodied by the following English sentences.\n",
      "Sentence: Why do amphibians take the bus? Because their cars are always getting toad.\n",
      "A. Simile\n",
      "B. Metaphor\n",
      "C. Personification\n",
      "D. Apostrophe\n",
      "E. Oxymoron\n",
      "F. Hyperbole\n",
      "G. Pun\n",
      "H. Euphemism\n",
      "I. Alliteration\n",
      "J. Onomatopoeia\n",
      "\n",
      "Figure of speech: \n",
      "OUTPUT::: G\n",
      "#################\n",
      "-------  tellmewhy|  --------- \n",
      "tellmewhy  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/tellmewhy/tellmewhy/task.json\n",
      "question: Why did he decide he will never go so long without seeing his best friend? \\n context: Rufus hasn't seen his best friend for three Year's. He decides it is finally time to visit his best friend. Rufus takes the long bus ride to Pennsylvania. When he is reunited with his best friend, Rufus is happy. He decides he will never go so long without seeing his best friend.\n",
      "answer: \n",
      "OUTPUT::: Rufus was happy to be reunited.\n",
      "#################\n",
      "-------  code_line_description|  --------- \n",
      "code_line_description  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/code_line_description/code_line_description/task.json\n",
      "\n",
      "Python code:\n",
      "def ascii_value(character):\n",
      "    return ord(character)\n",
      "ascii_value('a')\n",
      "\n",
      "A. computes the ordinary value of a\n",
      "B. computes the ascii value of character 'a'\n",
      "C. returns the character 'a'\n",
      "D. returns the sum of two numbers\n",
      "\n",
      "\n",
      "English language description:\n",
      "\n",
      "OUTPUT::: B\n",
      "#################\n",
      "-------  epistemic_reasoning|  --------- \n",
      "epistemic_reasoning  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/epistemic_reasoning/epistemic_reasoning/task.json\n",
      "Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\n",
      "\n",
      "Premise: Abigail suspects that a woman is walking her dog and using her cellphone. Hypothesis: Abigail suspects that the woman is using her cellphone.\n",
      "A. entailment\n",
      "B. non-entailment\n",
      "\n",
      "Relation: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  formal_fallacies_syllogisms_negation|  --------- \n",
      "formal_fallacies_syllogisms_negation  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/formal_fallacies_syllogisms_negation/task.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"Is Siri a stepsister of Mary? Is Susan related to Kate? In large families, it is sometimes difficult to keep track of all one's relatives. The following argument seeks to clarify some such relations: Whoever is neither a niece of Tonda nor a half-sister of Christina is a granddaughter of Sondra or a half-sister of Lynn.Every granddaughter of Sondra is a half-sister of Christina or a niece of Tonda. It follows that whoever is none of this: a niece of Tonda or half-sister of Christina, is a half-sister of Lynn.\"\n",
      " Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
      "A. valid\n",
      "B. invalid\n",
      "Answer: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  logic_grid_puzzle|  --------- \n",
      "logic_grid_puzzle  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logic_grid_puzzle/logic_grid_puzzle/task.json\n",
      "Question: There are 2 houses next to each other, numbered 1 on the left and 2 on the right. There is one person living in each house. The people in these houses have different characteristics:\n",
      " - Everyone likes a different kind of book: one is a romance book lover and one is a fantasy book enthusiast\n",
      " - Each person has a favorite color: one likes blue and one likes white\n",
      " - Each person has a different kind of car: one drives a minivan and one drives a convertible\n",
      "\n",
      "Clue(s):\n",
      "1. The fantasy book enthusiast lives in the first house.\n",
      "2. The person who likes blue does not live in the first house.\n",
      "3. The person who drives a minivan lives in the first house.\n",
      "\n",
      "What is the number of the house where the fantasy book enthusiast lives?\n",
      "A. 1\n",
      "B. 2\n",
      "Answer: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  logical_fallacy_detection|  --------- \n",
      "logical_fallacy_detection  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/logical_fallacy_detection/logical_fallacy_detection/task.json\n",
      "Question: This AI is identifying whether statements contain fallacies. The AI responds with 'Valid' or 'Invalid' as appropriate. Peter is younger than Amy. Peter is younger than Philipp. Philipp is younger than Amy. Therefore amy is older than Philipp.\n",
      "A. Valid\n",
      "B. Invalid\n",
      "Answer: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  object_counting|  --------- \n",
      "object_counting  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/object_counting/object_counting/task.json\n",
      "Question: This AI is identifying whether statements contain fallacies. The AI responds with 'Valid' or 'Invalid' as appropriate. Peter is younger than Amy. Peter is younger than Philipp. Philipp is younger than Amy. Therefore amy is older than Philipp.\n",
      "A. Valid\n",
      "B. Invalid\n",
      "Answer: \n",
      "OUTPUT::: A\n",
      "#################\n",
      "-------  presuppositions_as_nli|  --------- \n",
      "presuppositions_as_nli  ../01_helm_eval_setup/benchmark_output/scenarios/big_bench/presuppositions_as_nli/presuppositions_as_nli/task.json\n",
      "This is a natural language inference task. There are two sentences in English. The answer is \"entailment\" if the first sentence entails the second, \"contradiction\" if the second sentence contradicts the first, and \"neutral\" if neither is of those two cases holds.\n",
      "\n",
      "\n",
      "Sentence 1:   Streaks: Victor Santos hasn't failed to make it out of third inning in back-to-back starts. \n",
      "Sentence 2: It made Victor Santos go out of third inning in back-to-back starts.\n",
      "\n",
      "The answer is: \n",
      "A. entailment\n",
      "B. neutral\n",
      "C. contradiction\n",
      "\n",
      "\n",
      "OUTPUT::: B\n"
     ]
    }
   ],
   "source": [
    "whole_arr = []\n",
    "\n",
    "bb_id = -1\n",
    "for curr_section in ans_df:\n",
    "    print(\"#################\")\n",
    "    print(\"------- \" , curr_section, \" --------- \")\n",
    "    curr_task, curr_subtask = curr_section.split(\"|\")\n",
    "    if curr_subtask == \"\":\n",
    "        path_to_read = os.path.join(BASE_DIR, curr_task, \"task.json\")\n",
    "    else:\n",
    "        path_to_read = os.path.join(BASE_DIR, curr_task, curr_subtask, curr_task, curr_subtask, \"task.json\")\n",
    "    if not os.path.exists(path_to_read):\n",
    "        path_to_read = os.path.join(BASE_DIR, curr_task, curr_task,  \"task.json\")\n",
    "    print(curr_task, curr_subtask, path_to_read)\n",
    "    try:\n",
    "        with open(path_to_read, 'r') as fd:\n",
    "            inst_df = json.load(fd)\n",
    "    except:\n",
    "        print(\"SKIPPING.\")\n",
    "        print(path_to_read)\n",
    "        continue\n",
    "    ###########################\n",
    "    instructions=inst_df.get(\"task_prefix\", \"\")\n",
    "    input_prefix=inst_df.get(\"example_input_prefix\", \"Question: \")\n",
    "    output_prefix=inst_df.get(\"example_output_prefix\", \"Answer: \")\n",
    "    ############################\n",
    "    for _eg in ans_df[curr_section]:\n",
    "        bb_id+=1\n",
    "        obj = dict()\n",
    "        obj['bb_id'] = bb_id\n",
    "        obj['section'] = curr_section\n",
    "        obj['task'] = curr_task\n",
    "        obj['subtask'] = curr_subtask\n",
    "        obj['org_task'] = True if curr_task in bigbench_tasks else False\n",
    "        obj['split_used'] = _eg.split\n",
    "        input_str, correct_option = helper.fetch_general_task(_eg, instructions, input_prefix, output_prefix)\n",
    "        obj['instruction'] = input_str\n",
    "        obj['output'] = correct_option\n",
    "        whole_arr.append(obj)\n",
    "        #break\n",
    "    print(whole_arr[-1]['instruction'])\n",
    "    print(\"OUTPUT:::\", whole_arr[-1]['output'])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5069386",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee8fbef2",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f7c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bb_id': 2739,\n",
       " 'section': 'presuppositions_as_nli|',\n",
       " 'task': 'presuppositions_as_nli',\n",
       " 'subtask': '',\n",
       " 'org_task': False,\n",
       " 'split_used': 'test',\n",
       " 'instruction': 'This is a natural language inference task. There are two sentences in English. The answer is \"entailment\" if the first sentence entails the second, \"contradiction\" if the second sentence contradicts the first, and \"neutral\" if neither is of those two cases holds.\\n\\n\\nSentence 1:   Streaks: Victor Santos hasn\\'t failed to make it out of third inning in back-to-back starts. \\nSentence 2: It made Victor Santos go out of third inning in back-to-back starts.\\n\\nThe answer is: \\nA. entailment\\nB. neutral\\nC. contradiction\\n\\n',\n",
       " 'output': 'B'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_arr[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3c2e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2740"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eda5a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'train': 1608, 'test': 576, 'valid': 556})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x['split_used'] for x in whole_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ea6f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine whether the following pairs of sentences embody an entailment relation or not.\n",
      "\n",
      "Sentences: John is inside the room. Therefore the room is not empty.\n",
      "A. entailment\n",
      "B. no-entailment\n",
      "\n",
      "Relation: \n"
     ]
    }
   ],
   "source": [
    "print(whole_arr[0]['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "740f5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = list(filter(lambda x: \"train\" in x['split_used'], whole_arr))\n",
    "valid_arr = list(filter(lambda x: \"valid\" == x['split_used'], whole_arr))\n",
    "test_arr = list(filter(lambda x: \"test\" == x['split_used'], whole_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50c82f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1608\n",
      "Counter({'train': 1608})\n",
      "Counter({'A': 529, 'B': 465, 'C': 231, 'D': 127, 'E': 110, 'G': 65, 'F': 60, 'H': 2, 'J': 2, 'I': 1, 'they sprayed chemicals inside her house.': 1, 'He acted in a way they approved of.': 1, 'they were more like friends.': 1, 'he had finished all other laps.': 1, 'Besty had a sweet tooth.': 1, 'they wanted to make sure they were healthy after sleeping around.': 1, \"Rufus hadn't seen his friend in years.\": 1, 'Duke and Ellen felt that this was the best solution as Duke was interested in playing football and Ellen was interested in watching a movie.': 1, 'the salesman at the dealership told me that the sale was over yesterday.': 1, 'Matt and Jimmy were drawn to the creepiness of the old institution.': 1, 'his education was just right for the job.': 1, 'Ed missed Riley.': 1, 'she was very careful regarding her appearance.': 1, 'she was attending college.': 1, 'Jim wanted it to look like blood.': 1, 'Rufus was happy to be reunited.': 1})\n",
      "Counter({'logical_deduction|seven_objects': 420, 'logical_deduction|five_objects': 300, 'logical_deduction|three_objects': 180, 'causal_judgment|': 114, 'snarks|': 109, 'strange_stories|multiple_choice': 73, 'empirical_judgments|': 61, 'emoji_movie|': 60, 'dark_humor_detection|': 48, 'analytic_entailment|': 38, 'strange_stories|boolean': 21, 'formal_fallacies_syllogisms_negation|': 21, 'presuppositions_as_nli|': 21, 'logic_grid_puzzle|': 19, 'entailed_polarity|': 18, 'logical_fallacy_detection|': 18, 'code_line_description|': 17, 'figure_of_speech_detection|': 16, 'tellmewhy|': 16, 'epistemic_reasoning|': 15, 'known_unknowns|': 14, 'cause_and_effect|one_sentence': 9})\n"
     ]
    }
   ],
   "source": [
    "arr_look = train_arr\n",
    "print(len(arr_look))\n",
    "print(Counter([x['split_used'] for x in arr_look]))\n",
    "print(Counter([x['output'] for x in arr_look]))\n",
    "print(Counter([x['section'] for x in arr_look]))\n",
    "\n",
    "#print(Counter([x['perturb_type'] for x in arr_look]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4067813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556\n",
      "Counter({'valid': 556})\n",
      "Counter({'A': 187, 'B': 157, 'C': 66, 'D': 54, 'E': 46, 'F': 21, 'G': 19, 'I': 2, 'J': 1, 'They would be disappointed in him if he tried the cigarette.': 1, 'the dogs likes walking.': 1, 'he compared the two.': 1})\n",
      "Counter({'logical_deduction|seven_objects': 140, 'logical_deduction|five_objects': 100, 'logical_deduction|three_objects': 60, 'causal_judgment|': 38, 'snarks|': 36, 'strange_stories|multiple_choice': 24, 'emoji_movie|': 20, 'empirical_judgments|': 19, 'analytic_entailment|': 16, 'known_unknowns|': 16, 'strange_stories|boolean': 16, 'dark_humor_detection|': 16, 'cause_and_effect|one_sentence': 11, 'epistemic_reasoning|': 7, 'entailed_polarity|': 6, 'figure_of_speech_detection|': 5, 'code_line_description|': 5, 'formal_fallacies_syllogisms_negation|': 5, 'presuppositions_as_nli|': 5, 'logic_grid_puzzle|': 4, 'logical_fallacy_detection|': 4, 'tellmewhy|': 3})\n"
     ]
    }
   ],
   "source": [
    "arr_look = valid_arr\n",
    "print(len(arr_look))\n",
    "print(Counter([x['split_used'] for x in arr_look]))\n",
    "print(Counter([x['output'] for x in arr_look]))\n",
    "print(Counter([x['section'] for x in arr_look]))\n",
    "#print(Counter([x['perturb_type'] for x in arr_look]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "230f788f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "Counter({'test': 576})\n",
      "Counter({'A': 185, 'B': 150, 'C': 82, 'D': 60, 'E': 49, 'F': 19, 'G': 18, 'H': 1, 'J': 1, 'she loved peace.': 1, 'her mother called her grandma and let Kay chat with her for an hour.': 1, 'she was little and curious.': 1, 'James wanted to score a touchdown.': 1, 'it was her home.': 1, 'she was excited about seeing it.': 1, 'Justin lost his regular job.': 1, 'Justin was qualified and he wanted this job.': 1, 'it was even.': 1, 'he was done with high school.': 1, 'both had lot of free time on that day.': 1})\n",
      "Counter({'logical_deduction|seven_objects': 140, 'logical_deduction|five_objects': 100, 'logical_deduction|three_objects': 60, 'causal_judgment|': 38, 'snarks|': 36, 'strange_stories|multiple_choice': 24, 'emoji_movie|': 20, 'empirical_judgments|': 19, 'analytic_entailment|': 16, 'known_unknowns|': 16, 'strange_stories|boolean': 16, 'dark_humor_detection|': 16, 'tellmewhy|': 11, 'cause_and_effect|one_sentence': 10, 'figure_of_speech_detection|': 9, 'code_line_description|': 8, 'epistemic_reasoning|': 8, 'logical_fallacy_detection|': 8, 'logic_grid_puzzle|': 7, 'entailed_polarity|': 6, 'formal_fallacies_syllogisms_negation|': 4, 'presuppositions_as_nli|': 4})\n"
     ]
    }
   ],
   "source": [
    "arr_look = test_arr\n",
    "print(len(arr_look))\n",
    "print(Counter([x['split_used'] for x in arr_look]))\n",
    "print(Counter([x['output'] for x in arr_look]))\n",
    "print(Counter([x['section'] for x in arr_look]))\n",
    "#print(Counter([x['perturb_type'] for x in arr_look]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06734e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_arr)\n",
    "random.shuffle(valid_arr)\n",
    "random.shuffle(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ffc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_path  = \"../../data/new_training_datasets/pegasus_bigbench_train_dataset.json\"\n",
    "with open(save_path, 'w') as fd:\n",
    "    json.dump(train_arr, fd, indent=1)\n",
    "    \n",
    "save_path  = \"../../data/new_training_datasets/pegasus_bigbench_valid_dataset.json\"\n",
    "with open(save_path, 'w') as fd:\n",
    "    json.dump(valid_arr, fd, indent=1)\n",
    "    \n",
    "save_path  = \"../../data/new_training_datasets/pegasus_bigbench_test_dataset.json\"\n",
    "with open(save_path, 'w') as fd:\n",
    "    json.dump(test_arr, fd, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e461c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wizard_coder_kernel",
   "language": "python",
   "name": "wizard_coder_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
